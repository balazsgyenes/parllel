defaults:
  - sac@algo
  - _self_

parallel: True
batch_T: 128
batch_B: 16
max_steps_decorrelate: 50
env:
  max_episode_steps: 1000
  reward_type: sparse
device: "cpu"
algo:
  replay_length: 2560  # 20 iterations
  learning_starts: 10000
  replay_ratio: 64
  target_update_tau: 0.01
pi_model:
  hidden_sizes: [64, 64]
  hidden_nonlinearity: Tanh
q_model:
  hidden_sizes: [64, 64]
  hidden_nonlinearity: Tanh
distribution:
  deterministic_eval_mode: True
eval_sampler:
  max_traj_length: 2000
  max_trajectories: 40
  n_eval_envs: 16
runner:
  n_steps: 204800  # 100 iterations
  log_interval_steps: 10240  # log every 5 iterations
