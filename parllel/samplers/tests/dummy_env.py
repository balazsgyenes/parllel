import copy
import traceback
from typing import Tuple

import gym
from gym import spaces
from nptyping import NDArray

from parllel.arrays import Array, RotatingArray, buffer_from_dict_example
from parllel.buffers import Buffer, NamedTuple
from parllel.samplers import EnvSamples
from parllel.types import BatchSpec


class DummyEnv(gym.Env):
    def __init__(self, action_space: gym.Space, observation_space: gym.Space,
            episode_length: int, batch_spec: BatchSpec, n_batches: int,
            multi_reward: bool = False) -> None:
        self.observation_space = observation_space
        self.action_space = action_space
        if multi_reward:
            self.reward_space = spaces.Dict({
                "alice": spaces.Box(-10, 10, shape=()),
                "bob": spaces.Box(-10, 10, shape=()),
            })
        else:
            self.reward_space = spaces.Box(-10, 10, shape=())

        self._episode_length = episode_length

        # allocate buffers to store data generated by this env
        self._step_ctr = 0
        batch_observation = buffer_from_dict_example(self.observation_space.sample(),
            (n_batches * batch_spec.T,), RotatingArray, name="obs")
        batch_reward = buffer_from_dict_example(self.reward_space.sample(),
            (n_batches * batch_spec.T,), Array, name="reward")
        batch_done = buffer_from_dict_example(True,
            (n_batches * batch_spec.T,), Array, name="done")
        batch_info = buffer_from_dict_example({"action": self.action_space.sample()},
            (n_batches * batch_spec.T,), Array, name="envinfo")
        self._env_samples = EnvSamples(batch_observation, batch_reward, batch_done, batch_info)

    def step(self, action: NDArray) -> Tuple[Buffer, NDArray, bool, NamedTuple]:
        obs = self.observation_space.sample()
        reward = self.reward_space.sample()
        done = self._traj_counter >= self._episode_length
        env_info = {"action": copy.deepcopy(action)}

        names = [frame.name for frame in traceback.extract_stack()]
        if "random_step_async" in names or "get_example_output" in names:
            self._env_samples.observation[0] = obs
        else:
            self._env_samples.observation[self._step_ctr + 1] = obs
            self._env_samples.reward[self._step_ctr] = reward
            self._env_samples.done[self._step_ctr] = done
            self._env_samples.env_info[self._step_ctr] = env_info
            self._step_ctr += 1

        self._traj_counter += 1
        return (obs, reward, done, env_info)

    def reset(self) -> NDArray:
        self._traj_counter = 0
        obs = self.observation_space.sample()
        self._env_samples.observation[self._step_ctr] = obs
        return obs

    @property
    def env_samples(self):
        return self._env_samples
