import copy
import traceback
from typing import Tuple

import gym
from gym import spaces
from nptyping import NDArray

from parllel.arrays import Array, RotatingArray, buffer_from_dict_example
from parllel.arrays.utils import buffer_from_example
from parllel.buffers import Buffer, NamedTuple
from parllel.buffers import EnvSamples
from parllel.types import BatchSpec


class DummyEnv(gym.Env):
    def __init__(self, action_space: gym.Space, observation_space: gym.Space,
            episode_length: int, batch_spec: BatchSpec, n_batches: int,
            multireward: bool = False, wait_before_reset: bool = False) -> None:
        self.observation_space = observation_space
        self.action_space = action_space
        if multireward:
            self.reward_space = spaces.Dict({
                "alice": spaces.Box(-10, 10, shape=()),
                "bob": spaces.Box(-10, 10, shape=()),
            })
        else:
            self.reward_space = spaces.Box(-10, 10, shape=())

        self.episode_length = episode_length
        self.batch_spec = batch_spec
        self.wait_before_reset = wait_before_reset

        # allocate buffers to store data generated by this env
        self._step_ctr = 0
        batch_observation = buffer_from_dict_example(self.observation_space.sample(),
            (n_batches * batch_spec.T,), RotatingArray, name="obs")
        batch_reward = buffer_from_dict_example(self.reward_space.sample(),
            (n_batches * batch_spec.T,), Array, name="reward")
        batch_done = buffer_from_dict_example(True,
            (n_batches * batch_spec.T,), Array, name="done")
        batch_info = buffer_from_dict_example({"action": self.action_space.sample()},
            (n_batches * batch_spec.T,), Array, name="envinfo")
        self._samples = EnvSamples(batch_observation, batch_reward, batch_done, batch_info)
        self._batch_resets = buffer_from_example(True,
            (n_batches * batch_spec.T,), RotatingArray, padding=1)

    def step(self, action: NDArray) -> Tuple[Buffer, NDArray, bool, NamedTuple]:
        obs = self.observation_space.sample()
        reward = self.reward_space.sample()
        done = self._traj_counter >= self.episode_length
        env_info = {"action": copy.deepcopy(action)}

        # check the call stack to determine if this is a "real sample" or not
        # if just getting example or decorrelating, keep overwriting "reset"
        # observation
        names = [frame.name for frame in traceback.extract_stack()]
        if "random_step_async" in names:
            self._samples.observation[0] = obs
        else:
            self._samples.observation[self._step_ctr + 1] = obs
            self._samples.reward[self._step_ctr] = reward
            self._samples.done[self._step_ctr] = done
            self._samples.env_info[self._step_ctr] = env_info
            self._step_ctr += 1

        self._traj_counter += 1
        return (obs, reward, done, env_info)

    def reset(self) -> NDArray:
        self._traj_counter = 1
        self._batch_resets[self._step_ctr - 1] = True
        if self.wait_before_reset:
            # sampling batch may have stopped early
            batch_ctr = (self._step_ctr - 1) // self.batch_spec.T + 1
            # advance counter to the next batch
            self._step_ctr = batch_ctr * self.batch_spec.T
        obs = self.observation_space.sample()
        self._samples.observation[self._step_ctr] = obs
        return obs

    @property
    def samples(self):
        return self._samples

    @property
    def resets(self):
        return self._batch_resets
