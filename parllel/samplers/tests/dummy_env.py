# from itertools import count
from typing import Tuple

import gym
from gym import spaces
from nptyping import NDArray

from parllel.arrays import Array, RotatingArray, buffer_from_dict_example
from parllel.buffers import Buffer, NamedTuple, buffer_method
from parllel.samplers import EnvSamples


class DummyEnv(gym.Env):
    def __init__(self, observation_space: gym.Space, action_space: gym.Space, episode_length: int, batch_T: int,
            multi_reward: bool = False) -> None:
        self.observation_space = observation_space
        self.action_space = action_space
        if multi_reward:
            self.reward_space = spaces.Box(-10, 10, shape=())
        else:
            self.reward_space = spaces.Dict({
                "alice": spaces.Box(-10, 10, shape=()),
                "bob": spaces.Box(-10, 10, shape=()),
            })

        self._episode_length = episode_length

        # allocate buffers to store data generated by this env
        self._total_steps = -1
        batch_observation = buffer_from_dict_example(self.observation_space.sample,
            (batch_T,), RotatingArray, name="obs")
        batch_reward = buffer_from_dict_example(self.reward_space.sample(),
            (batch_T,), Array, name="reward")
        batch_done = buffer_from_dict_example(True, (batch_T,), Array, name="done")
        batch_info = buffer_from_dict_example({"action": self.action_space.sample()},
            (batch_T,), Array, name="envinfo")
        self._env_samples = EnvSamples(batch_observation, batch_reward, batch_done, batch_info)

    def step(self, action: NDArray) -> Tuple[Buffer, NDArray, bool, NamedTuple]:
        self._traj_counter += 1
        self._total_steps += 1
        obs = self.observation_space.sample()
        reward = self.reward_space.sample()
        done = self._traj_counter >= self._episode_length
        env_info = {"action": buffer_method(action, "copy")}
        self._env_samples.observation[self._total_steps + 1] = obs
        self._env_samples.reward[self._total_steps] = reward
        self._env_samples.done[self._total_steps] = done
        self._env_samples.env_info[self._total_steps] = env_info
        # env_step = {
        #     "obs": self.observation_space.sample(),
        #     "reward": self.reward_space.sample(),
        #     "done": self._traj_counter >= self._episode_length,
        #     "env_info": {"action": buffer_method(action, "copy")},
        # }
        # self._env_samples[self._total_steps] = env_step
        return (obs, reward, done, env_info)

    def reset(self) -> NDArray:
        self._traj_counter = 0
        obs = self.observation_space.sample()
        self._env_samples.observation[self._total_steps + 1] = obs
        return obs

    @property
    def env_samples(self):
        return self._env_samples
