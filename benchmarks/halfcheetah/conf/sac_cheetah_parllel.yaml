defaults:
  - _self_

framework: parllel
parallel: True
batch_T: 1
batch_B: 8
device: null
pi_model:
  hidden_sizes: [256, 256]  # from Haarnoja et al.
  hidden_nonlinearity: ReLU  # from Haarnoja et al.
q_model:
  hidden_sizes: [256, 256]  # from Haarnoja et al.
  hidden_nonlinearity: ReLU  # from Haarnoja et al.
distribution:
  deterministic_eval_mode: False
algo:
  learning_rate: 7.3e-4  # from rl-baselines3-zoo
  replay_size: 300000  # from rl-baselines3-zoo
  batch_size: 256  # from Haarnoja et al.
  ent_coeff: 0.005  # cannot use auto, as parllel doesn't support this
  discount: 0.98  # from rl-baselines3-zoo
  target_update_tau: 0.02  # from rl-baselines3-zoo
  replay_ratio: 256  # from rl-baselines3-zoo
  learning_starts: 10000  # from rl-baselines3-zoo
  random_explore_steps: 10000  # from rl-baselines3-zoo
  target_update_interval: 1
  clip_grad_norm: null
eval_sampler:
  max_traj_length: 2000
  max_trajectories: 40
  n_eval_envs: 16
runner:
  n_steps: 1.e+6
  log_interval_steps: 50000
  eval_interval_steps: 50000


# TODO:
# double check if train_freq is real steps or vectorized steps
# check policy architectures
# entropy coefficient
# clip grad norm?